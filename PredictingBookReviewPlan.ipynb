{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Implement Your Machine Learning Project Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab assignment, you will implement the machine learning project plan you created in the written assignment. You will:\n",
    "\n",
    "1. Load your data set and save it to a Pandas DataFrame.\n",
    "2. Perform exploratory data analysis on your data to determine which feature engineering and data preparation techniques you will use.\n",
    "3. Prepare your data for your model and create features and a label.\n",
    "4. Fit your model to the training data and evaluate your model.\n",
    "5. Improve your model by performing model selection and/or feature selection techniques to find best model for your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "\n",
    "Before you get started, import a few packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> In the code cell below, import additional packages that you have used in this course that you will need for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, log_loss, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load the Data Set\n",
    "\n",
    "\n",
    "You have chosen to work with one of four data sets. The data sets are located in a folder named \"data.\" The file names of the three data sets are as follows:\n",
    "\n",
    "* The \"adult\" data set that contains Census information from 1994 is located in file `adultData.csv`\n",
    "* The airbnb NYC \"listings\" data set is located in file  `airbnbListingsData.csv`\n",
    "* The World Happiness Report (WHR) data set is located in file `WHR2018Chapter2OnlineData.csv`\n",
    "* The book review data set is located in file `bookReviewsData.csv`\n",
    "\n",
    "\n",
    "\n",
    "<b>Task:</b> In the code cell below, use the same method you have been using to load your data using `pd.read_csv()` and save it to DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Positive Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This was perhaps the best of Johannes Steinhof...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This very fascinating book is a story written ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The four tales in this collection are beautifu...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The book contained more profanity than I expec...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We have now entered a second time of deep conc...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Positive Review\n",
       "0  This was perhaps the best of Johannes Steinhof...             True\n",
       "1  This very fascinating book is a story written ...             True\n",
       "2  The four tales in this collection are beautifu...             True\n",
       "3  The book contained more profanity than I expec...            False\n",
       "4  We have now entered a second time of deep conc...             True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = os.path.join(os.getcwd(), \"data\", \"bookReviewsData.csv\")\n",
    "df = pd.read_csv(filename, header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exploratory Data Analysis\n",
    "\n",
    "The next step is to inspect and analyze your data set with your machine learning problem and project plan in mind. \n",
    "\n",
    "This step will help you determine data preparation and feature engineering techniques you will need to apply to your data to build a balanced modeling data set for your problem and model. These data preparation techniques may include:\n",
    "* addressing missingness, such as replacing missing values with means\n",
    "* renaming features and labels\n",
    "* finding and replacing outliers\n",
    "* performing winsorization if needed\n",
    "* performing one-hot encoding on categorical features\n",
    "* performing vectorization for an NLP problem\n",
    "* addressing class imbalance in your data sample to promote fair AI\n",
    "\n",
    "\n",
    "Think of the different techniques you have used to inspect and analyze your data in this course. These include using Pandas to apply data filters, using the Pandas `describe()` method to get insight into key statistics for each column, using the Pandas `dtypes` property to inspect the data type of each column, and using Matplotlib and Seaborn to detect outliers and visualize relationships between features and labels. If you are working on a classification problem, use techniques you have learned to determine if there is class imbalance.\n",
    "\n",
    "\n",
    "<b>Task</b>: Use the techniques you have learned in this course to inspect and analyze your data. \n",
    "\n",
    "<b>Note</b>: You can add code cells if needed by going to the <b>Insert</b> menu and clicking on <b>Insert Cell Below</b> in the drop-drown menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of X: (1973,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    This was perhaps the best of Johannes Steinhof...\n",
       "1    This very fascinating book is a story written ...\n",
       "2    The four tales in this collection are beautifu...\n",
       "3    The book contained more profanity than I expec...\n",
       "4    We have now entered a second time of deep conc...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create labeled examples\n",
    "y = df['Positive Review']\n",
    "X = df['Review']\n",
    "print('The size of X: ' + str(X.shape))\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution: False    993\n",
      "True     980\n",
      "Name: Positive Review, dtype: int64\n",
      "\n",
      "Class Ratios: False    0.503294\n",
      "True     0.496706\n",
      "Name: Positive Review, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check if class imbalance existed\n",
    "class_distribution = y.value_counts()\n",
    "print('Class Distribution: ' + str(class_distribution))\n",
    "\n",
    "# Calculate class ratios\n",
    "class_ratios = class_distribution / len(df)\n",
    "print('\\nClass Ratios: ' + str(class_ratios))\n",
    "\n",
    "# Class imbalance occurs when there is a significant disparity in the number \n",
    "# of instandces between different classes. \n",
    "# In this case, since the class ratios are quite close(0.503 & 0.497), \n",
    "# there is a relatively balanced class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Positive Review:\n",
      " The four tales in this collection are beautifully composed; they are art, not just stories.  Each story is deep in its unique complexities.  Each one has plots and subplots and paints an impeccable image of the story upon the reader's mind.  And when I look back upon the book as a whole, upon the adventurous stories, the excitement and emotion that the author presents so exquisitely, I can't help but be extremely impressed.\n",
      "\n",
      "A Negative Review:\n",
      " The book contained more profanity than I expected to read in a book by Rita Rudner.  I had expected more humor from a comedienne.  Too bad, because I really like her humor\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at an example of a positive and a negative review\n",
    "print('A Positive Review:\\n', X[2])\n",
    "print('A Negative Review:\\n', X[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Implement Your Project Plan\n",
    "\n",
    "<b>Task:</b> Use the rest of this notebook to carry out your project plan. You will:\n",
    "\n",
    "1. Prepare your data for your model and create features and a label.\n",
    "2. Fit your model to the training data and evaluate your model.\n",
    "3. Improve your model by performing model selection and/or feature selection techniques to find best model for your problem.\n",
    "\n",
    "\n",
    "Add code cells below and populate the notebook with commentary, code, analyses, results, and figures as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500     There is a reason this book has sold over 180,...\n",
       "1047    There is one thing that every cookbook author ...\n",
       "1667    Being an engineer in the aerospace industry I ...\n",
       "1646    I have no idea how this book has received the ...\n",
       "284     It is almost like dream comes true when I saw ...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split labeled examples into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, \n",
    "                                                    random_state=1234)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 18558: \n",
      "[('there', 16673), ('is', 9043), ('reason', 13533), ('this', 16714), ('book', 2189), ('has', 7803), ('sold', 15423), ('over', 11793), ('180', 73), ('000', 1), ('copies', 3867), ('it', 9076), ('gets', 7240), ('right', 14207), ('to', 16835), ('the', 16627), ('point', 12568), ('accompanies', 444), ('each', 5372), ('strategy', 15943), ('with', 18277), ('visual', 17844), ('aid', 750), ('so', 15386), ('you', 18497), ('can', 2604), ('get', 7239), ('mental', 10534), ('picture', 12402), ('in', 8491), ('your', 18501), ('head', 7844), ('further', 7051), ('its', 9088), ('section', 14743), ('on', 11601), ('analyzing', 974), ('stocks', 15886), ('and', 984), ('commentary', 3384), ('state', 15782), ('of', 11543), ('financial', 6568), ('statements', 15786), ('market', 10286), ('are', 1220), ('money', 10863), ('if', 8336), ('just', 9282), ('starting', 15774)]\n",
      "\n",
      "[[0.         0.16185315 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.01923341 0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Implement TF-IDF vectorizer to transform text\n",
    "# 1. Create a TfidfVectorizer oject\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 2. Fit the vectorizer to X_train\n",
    "tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "# 3. Print the first 50 items in the vocabulary\n",
    "print(\"Vocabulary size {0}: \".format(len(tfidf_vectorizer.vocabulary_)))\n",
    "print(str(list(tfidf_vectorizer.vocabulary_.items())[0:50])+'\\n')\n",
    "\n",
    "      \n",
    "# 4. Transform *both* the training and test data using the fitted vectorizer and its 'transform' attribute\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# 5. Print the matrix\n",
    "print(X_train_tfidf.todense())\n",
    "\n",
    "# 6. Print the matrix\n",
    "print(X_test_tfidf.todense())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss of the Logistic Regression model: 0.6688500721532677\n",
      "Accuracy score of the Logistic Regression model: 0.6072874493927125\n",
      "AUC on the test data: 0.6435\n",
      "Mean Cross Validation on the test data: 0.5779984229539844\n",
      "The size of the feature space: 9\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('this', 7), ('book', 1), ('it', 4), ('to', 8)]:\n"
     ]
    }
   ],
   "source": [
    "# Fit a logistic regression model to the transformed training data \n",
    "# and evaluate the model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the transformed test data using the predit_proba() method\n",
    "probability_predictions = model.predict_proba(X_test_tfidf)[:,1]\n",
    "loss = log_loss(y_test, probability_predictions)\n",
    "print('Log loss of the Logistic Regression model: ' + str(loss))\n",
    "\n",
    "# Make predictions on the transformed test data using the predict() method\n",
    "class_label_predictions = model.predict(X_test_tfidf)\n",
    "acc_score = accuracy_score(y_test, class_label_predictions)\n",
    "print('Accuracy score of the Logistic Regression model: ' + str(acc_score))\n",
    "\n",
    "\n",
    "# Computer the area under the ROC curve for the test data.\n",
    "auc = roc_auc_score(y_test, probability_predictions)\n",
    "print('AUC on the test data: {:.4f}'.format(auc))\n",
    "\n",
    "# Computer the cross validation score\n",
    "cv_scores = cross_val_score(model, X_train_tfidf, y_train, cv=5, scoring='roc_auc')\n",
    "mean_cv_scores = np.mean(cv_scores)\n",
    "print('Mean Cross Validation on the test data:', mean_cv_scores)\n",
    "\n",
    "len_feature_space = len(tfidf_vectorizer.vocabulary_)\n",
    "print('The size of the feature space: {0}'.format(len_feature_space))\n",
    "\n",
    "# Get a glimpse of the features\n",
    "first_five =  list(tfidf_vectorizer.vocabulary_.items())[1:5]\n",
    "print('Glimpse of first 5 entries of the mapping of a word to its column/feature index \\n{}:'\n",
    "     .format(first_five))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review #1:\n",
      "\n",
      "I have read other books by Alesia Holliday and enjoyed them so I looked forward to reading this book.  Unfortunately, I could not get any farther than the first 25 pages.  I even tried diving in further into the book to see if it got better and I still could not read more than 5 pages without turning away.  The best I can do to pin down why I dislike it so much is to say that it tries too hard.  No character seems to even approach reality.  They are all, including the main character and her love interest, over the top\n",
      "\n",
      "\n",
      "Prediction: Is this a good review? False\n",
      "\n",
      "Actual: Is this a good review?False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# random test the model\n",
    "print('Review #1:\\n')\n",
    "print(X_test.to_numpy()[238])\n",
    "\n",
    "print('\\nPrediction: Is this a good review? {}\\n'.format(class_label_predictions[238]))\n",
    "\n",
    "print('Actual: Is this a good review?{}\\n'.format(y_test.to_numpy()[238]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Min Document Frequency Value: 1\n",
      "Log loss of the Logistic Regression model: 0.5595754586404746\n",
      "Accuracy score of the Logistic Regression model: 0.8562753036437247\n",
      "AUC on the test data: 0.9268\n",
      "Mean Cross Validation on the test data: 0.8963567733060099\n",
      "The size of the feature space: 138486\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('is', 61671), ('reason', 97323), ('this', 120815), ('book', 18054)]\n",
      "Glimpse of first 5 stop words \n",
      "[]\n",
      "\n",
      "Min Document Frequency Value: 10\n",
      "Log loss of the Logistic Regression model: 0.4970517918147751\n",
      "Accuracy score of the Logistic Regression model: 0.8380566801619433\n",
      "AUC on the test data: 0.9195\n",
      "Mean Cross Validation on the test data: 0.898919599323726\n",
      "The size of the feature space: 4023\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('is', 1687), ('reason', 2699), ('this', 3396), ('book', 464)]\n",
      "Glimpse of first 5 stop words \n",
      "['sagan fans', 'almoxt', 'grisham', 'kanji previously']\n",
      "\n",
      "Min Document Frequency Value: 100\n",
      "Log loss of the Logistic Regression model: 0.5163498953526879\n",
      "Accuracy score of the Logistic Regression model: 0.757085020242915\n",
      "AUC on the test data: 0.8463\n",
      "Mean Cross Validation on the test data: 0.826084967511742\n",
      "The size of the feature space: 257\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('is', 102), ('this', 207), ('book', 35), ('has', 81)]\n",
      "Glimpse of first 5 stop words \n",
      "['sagan fans', 'almoxt', 'grisham', 'kanji previously']\n",
      "\n",
      "Min Document Frequency Value: 1000\n",
      "Log loss of the Logistic Regression model: 0.6688500721532677\n",
      "Accuracy score of the Logistic Regression model: 0.6072874493927125\n",
      "AUC on the test data: 0.6435\n",
      "Mean Cross Validation on the test data: 0.5779984229539844\n",
      "The size of the feature space: 9\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('this', 7), ('book', 1), ('it', 4), ('to', 8)]\n",
      "Glimpse of first 5 stop words \n",
      "['sagan fans', 'almoxt', 'grisham', 'kanji previously']\n"
     ]
    }
   ],
   "source": [
    "# Experiment with Different Document Frequency Values and Analyze the Results\n",
    "for min_df in [1, 10, 100, 1000]:\n",
    "    print('\\nMin Document Frequency Value: {0}'.format(min_df))\n",
    "    \n",
    "    #1. Create a TfidfVectorizer object\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=min_df, ngram_range=(1,2))\n",
    "    \n",
    "    #2. Fit the vectorizer toX_train\n",
    "    tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "    #3. Transform the training and testing data\n",
    "    X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    \n",
    "    #4. Fit a logistic regression model to the transformed training data \n",
    "    # and evaluate the model\n",
    "    model = LogisticRegression(max_iter=200)\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    #5. Make predictions on the transformed test data\n",
    "    probability_predictions = model.predict_proba(X_test_tfidf)[:,1]\n",
    "    loss = log_loss(y_test, probability_predictions)\n",
    "    print('Log loss of the Logistic Regression model: ' + str(loss))\n",
    "    \n",
    "    #6. Make predictions on the transformed test data using the predict() method\n",
    "    class_label_predictions = model.predict(X_test_tfidf)\n",
    "    acc_score = accuracy_score(y_test, class_label_predictions)\n",
    "    print('Accuracy score of the Logistic Regression model: ' + str(acc_score))\n",
    "    \n",
    "    #7. Compute the Area under the ROC curve for the test data\n",
    "    auc = roc_auc_score(y_test, probability_predictions)\n",
    "    print('AUC on the test data: {:.4f}'.format(auc))\n",
    "    \n",
    "    #8. Computer the cross validation score\n",
    "    cv_scores = cross_val_score(model, X_train_tfidf, y_train, cv=5, scoring='roc_auc')\n",
    "    mean_cv_scores = np.mean(cv_scores)\n",
    "    print('Mean Cross Validation on the test data:', mean_cv_scores)\n",
    "\n",
    "    #9. Computer the size of the resulting feature spacing\n",
    "    len_feature_space = len(tfidf_vectorizer.vocabulary_)\n",
    "    print('The size of the feature space: {0}'.format(len_feature_space))\n",
    "    \n",
    "    #10. Get a glimpse of the features:\n",
    "    first_five = list(tfidf_vectorizer.vocabulary_.items())[1:5]\n",
    "    print('Glimpse of first 5 entries of the mapping of a word to its column/feature index \\n{}'\n",
    "         .format(first_five))\n",
    "    \n",
    "    #11. Print the first five \"stop words\"\n",
    "    first_five_stop = list(tfidf_vectorizer.stop_words_)[1:5]\n",
    "    print('Glimpse of first 5 stop words \\n{}'.format(first_five_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. The model with min_df=1 has the highest AUC(0.9268) and the highest \n",
    "#   cross_validation (0.8964),indicating excellent predictive performance and\n",
    "#   generalization across different folds of the training data. \n",
    "#   It also has a relatively good accuracy(0.8563) and a reasonable \n",
    "#   log loss(0.5596), indicating well-calibrated probabilities. \n",
    "#2. The model with min_df=10 follows closely with a high AUC(0.9195) and a similar\n",
    "#   mean cross_validation(0.8989), which also having a lower log loss(0.4971).\n",
    "#3. The models with min_df=100 and min_df=1000 have decreasing AUC and \n",
    "#   cross_validation, indicating decreasing discrimination power in \n",
    "#   distinguishing between classes. \n",
    "#4. Overall, based on the testing outputs, the model with min_df=1 seems to be \n",
    "#   the best performer, as it consistently demonstrates the highest AUC and \n",
    "#   mean cross_validation, along with reasonable accuuracy and log loss scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
